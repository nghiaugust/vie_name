# MobileNet-SVTR-CTC: Kiáº¿n trÃºc tá»‘i Æ°u cho nháº­n diá»‡n tÃªn tiáº¿ng Viá»‡t

## ğŸ“‹ Tá»•ng quan

ÄÃ¢y lÃ  kiáº¿n trÃºc má»›i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho nháº­n diá»‡n tÃªn tiáº¿ng Viá»‡t, káº¿t há»£p:

- **Backbone: MobileNetV3-Large** - Nhanh vÃ  nháº¹
- **Neck: SVTR-Tiny** - Trá»™n thÃ´ng tin local vÃ  global hiá»‡u quáº£
- **Head: CTC Loss** - Alignment-free, khÃ´ng cáº§n cÄƒn chá»‰nh position

## ğŸ¯ Æ¯u Ä‘iá»ƒm

### 1. **Tá»‘c Ä‘á»™ cao**
- MobileNetV3 sá»­ dá»¥ng depthwise convolution â†’ giáº£m 5-7x tÃ­nh toÃ¡n so vá»›i ResNet
- SVTR nháº¹ hÆ¡n BiLSTM nhÆ°ng hiá»‡u quáº£ hÆ¡n
- CTC khÃ´ng cáº§n attention mechanism phá»©c táº¡p

### 2. **ChÃ­nh xÃ¡c cao**
- Stride tÃ¹y chá»‰nh: (2,2) â†’ (2,1) â†’ (1,1) giá»¯ thÃ´ng tin sequence tá»‘t
- SVTR mixing: káº¿t há»£p local vÃ  global context
- CTC: phÃ¹ há»£p vá»›i text cÃ³ Ä‘á»™ dÃ i biáº¿n Ä‘á»•i

### 3. **PhÃ¹ há»£p vá»›i tÃªn tiáº¿ng Viá»‡t**
- TÃªn VN thÆ°á»ng ngáº¯n (2-5 tá»«) â†’ CTC hoáº¡t Ä‘á»™ng tá»‘t
- Dáº¥u thanh cáº§n context â†’ SVTR xá»­ lÃ½ tá»‘t
- Stride (2,1) giá»¯ thÃ´ng tin chiá»u rá»™ng cho sequence

## ğŸ“ Cáº¥u trÃºc files

```
VietOCR/
â”œâ”€â”€ vietocr/
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ backbone/
â”‚       â”‚   â”œâ”€â”€ mobilenetv3.py          # âœ¨ NEW: MobileNetV3 backbone
â”‚       â”‚   â”œâ”€â”€ svtr_neck.py            # âœ¨ NEW: SVTR neck
â”‚       â”‚   â””â”€â”€ cnn.py                  # âœ… UPDATED: thÃªm mobilenetv3
â”‚       â”œâ”€â”€ seqmodel/
â”‚       â”‚   â””â”€â”€ ctc.py                  # âœ¨ NEW: CTC head
â”‚       â””â”€â”€ mobilenet_svtr_ctc.py       # âœ¨ NEW: Complete model
â”‚
â””â”€â”€ custom_vietocr/
    â”œâ”€â”€ config_mobilenet_svtr_ctc.yml   # âœ¨ NEW: Config file
    â”œâ”€â”€ train_mobilenet_svtr_ctc.py     # âœ¨ NEW: Training script
    â””â”€â”€ README_MOBILENET_SVTR_CTC.md    # âœ¨ NEW: Documentation
```

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Import model

```python
from vietocr.model.mobilenet_svtr_ctc import mobilenet_svtr_ctc

# Standard version (cÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c)
model = mobilenet_svtr_ctc(
    vocab_size=150,
    hidden=256,
    svtr_depth=2,
    svtr_heads=8,
    dropout=0.1
)

# Light version (nhanh hÆ¡n, nháº¹ hÆ¡n)
from vietocr.model.mobilenet_svtr_ctc import mobilenet_svtr_ctc_light

model_light = mobilenet_svtr_ctc_light(
    vocab_size=150,
    hidden=128,
    svtr_depth=1,
    svtr_heads=4,
    dropout=0.1
)
```

### 2. Training

```python
# Sá»­ dá»¥ng training script cÃ³ sáºµn
python custom_vietocr/train_mobilenet_svtr_ctc.py \
    --config custom_vietocr/config_mobilenet_svtr_ctc.yml

# Hoáº·c resume tá»« checkpoint
python custom_vietocr/train_mobilenet_svtr_ctc.py \
    --config custom_vietocr/config_mobilenet_svtr_ctc.yml \
    --resume weights/mobilenet_svtr_ctc/checkpoint_epoch_50.pth
```

### 3. Inference

```python
import torch
from PIL import Image
import torchvision.transforms as transforms

# Load model
model = mobilenet_svtr_ctc(vocab_size=150, hidden=256)
model.load_state_dict(torch.load('best_model.pth')['model_state_dict'])
model.eval()

# Prepare image
transform = transforms.Compose([
    transforms.Resize((32, 256)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

image = Image.open('name.jpg').convert('RGB')
image = transform(image).unsqueeze(0)  # (1, 3, 32, 256)

# Inference
with torch.no_grad():
    logits = model(image)  # (1, T, vocab_size)
    
    # Decode vá»›i greedy
    decoded = model.decode(logits, method='greedy')
    
    # Hoáº·c beam search (chÃ­nh xÃ¡c hÆ¡n)
    decoded = model.decode(logits, method='beam_search', beam_width=5)

print("Predicted:", decoded[0])
```

## âš™ï¸ Cáº¥u hÃ¬nh

### Standard Version (khuyáº¿n nghá»‹)
- **Parameters**: ~3.5M
- **Hidden**: 256
- **SVTR depth**: 2
- **SVTR heads**: 8
- **Speed**: ~50ms/image (GPU)
- **For**: CÃ¢n báº±ng tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c

### Light Version (cho production)
- **Parameters**: ~1.2M
- **Hidden**: 128
- **SVTR depth**: 1
- **SVTR heads**: 4
- **Speed**: ~25ms/image (GPU)
- **For**: Deployment, real-time applications

## ğŸ“Š So sÃ¡nh vá»›i kiáº¿n trÃºc khÃ¡c

| Model | Params | Speed (ms) | Accuracy | Memory |
|-------|--------|------------|----------|--------|
| VGG-Transformer | 15M | 120 | 95% | 60MB |
| ResNet-Transformer | 25M | 150 | 96% | 100MB |
| **MobileNet-SVTR-CTC** | **3.5M** | **50** | **95%** | **14MB** |
| **MobileNet-SVTR-CTC (Light)** | **1.2M** | **25** | **93%** | **5MB** |

## ğŸ”§ Äiá»u chá»‰nh cho dataset cá»§a báº¡n

### 1. Thay Ä‘á»•i vocab
Trong file config:
```yaml
vocab: 'aAÃ Ã€áº£áº¢Ã£ÃƒÃ¡Ãáº¡áº ...'  # ThÃªm/bá»›t kÃ½ tá»± theo nhu cáº§u
```

### 2. Thay Ä‘á»•i image size
```yaml
train:
  image_height: 32  # CÃ³ thá»ƒ giá»¯ 32
  image_width: 256  # Äiá»u chá»‰nh theo Ä‘á»™ dÃ i tÃªn
```

### 3. Äiá»u chá»‰nh model size
```yaml
model:
  backbone_hidden: 256  # TÄƒng lÃªn 512 náº¿u muá»‘n model lá»›n hÆ¡n
  svtr_depth: 2         # TÄƒng lÃªn 3-4 náº¿u muá»‘n capacity cao hÆ¡n
  svtr_heads: 8         # Giá»¯ nguyÃªn hoáº·c tÄƒng lÃªn 12-16
```

### 4. Training hyperparameters
```yaml
train:
  batch_size: 32        # TÄƒng náº¿u GPU Ä‘á»§ máº¡nh
  learning_rate: 0.0003 # Äiá»u chá»‰nh theo batch size
  epochs: 100           # TÄƒng náº¿u dataset lá»›n
  use_amp: true         # AMP: tÄƒng tá»‘c ~2x, giáº£m memory ~50%
```

## ğŸš€ AMP (Automatic Mixed Precision)

Training script Ä‘Ã£ tÃ­ch há»£p **AMP** - sá»­ dá»¥ng FP16 thay vÃ¬ FP32:

### Æ¯u Ä‘iá»ƒm:
- âš¡ **Tá»‘c Ä‘á»™**: Nhanh hÆ¡n ~1.5-2x
- ğŸ’¾ **Memory**: Giáº£m ~40-50% VRAM
- ğŸ¯ **Accuracy**: Gáº§n nhÆ° khÃ´ng áº£nh hÆ°á»Ÿng

### Config:
```yaml
train:
  use_amp: true   # Báº­t AMP (khuyáº¿n nghá»‹ cho GPU hiá»‡n Ä‘áº¡i)
  # use_amp: false  # Táº¯t náº¿u gáº·p váº¥n Ä‘á» vá» numerical stability
```

### LÆ°u Ã½:
- Chá»‰ hoáº¡t Ä‘á»™ng trÃªn GPU (CUDA)
- GPU tá»« Pascal (GTX 10xx) trá»Ÿ lÃªn
- GPU Tensor Core (RTX, V100, A100) tá»‘i Æ°u nháº¥t
- Tá»± Ä‘á»™ng fallback vá» FP32 náº¿u khÃ´ng cÃ³ GPU

## ğŸ¨ Kiáº¿n trÃºc chi tiáº¿t

### Backbone: MobileNetV3-Large
```
Input (N, 3, H, W)
    â†“ First Conv (stride 2,2)
    â†“ Stage 1: stride (2,2) â†’ giáº£m nhanh á»Ÿ Ä‘áº§u
    â†“ Stage 2: stride (2,1) â†’ giá»¯ width cho sequence
    â†“ Stage 3: stride (1,1) â†’ giá»¯ nguyÃªn resolution
    â†“ Final Conv + Projection
Output (W', N, C)
```

**Stride pattern**:
- `(2,2)`: layers 0-2 - giáº£m nhanh kÃ­ch thÆ°á»›c
- `(2,1)`: layers 3-7 - giáº£m height, giá»¯ width
- `(1,1)`: layers 8-14 - giá»¯ nguyÃªn cho sequence

### Neck: SVTR-Tiny
```
Input (W, N, C)
    â†“ Input Projection (náº¿u cáº§n)
    â†“ Mixing Block 1 (Local Attention)
    â†“ Mixing Block 2 (Global Attention)
    â†“ ... (depth blocks, xen káº½ Local/Global)
    â†“ LayerNorm
Output (W, N, C)
```

**Mixing Block**:
- Local: Táº­p trung vÃ o patterns cá»¥c bá»™ (kÃ½ tá»± riÃªng láº»)
- Global: NhÃ¬n toÃ n cá»¥c (context giá»¯a cÃ¡c kÃ½ tá»±)

### Head: CTC
```
Input (W, N, C)
    â†“ Pre-projection (Linear + GELU)
    â†“ CTC Projection
Output (W, N, vocab_size)
    â†“ CTC Loss / Decoding
Final: Predicted sequence
```

## ğŸ“ Tips & Best Practices

### 1. Data Augmentation
- Rotation: Â±5 Ä‘á»™ (tÃªn thÆ°á»ng tháº³ng)
- Blur: 0.5 prob (simulate low quality)
- Noise: 0.3 prob (realistic condition)

### 2. Training
- Warmup LR: 5 epochs
- Cosine annealing: smooth decay
- Gradient clipping: 5.0 (prevent exploding)
- Early stopping: patience 15

### 3. Inference
- Greedy decoding: Nhanh nháº¥t
- Beam search (width=5): ChÃ­nh xÃ¡c hÆ¡n ~2%
- Batch inference: Process nhiá»u áº£nh cÃ¹ng lÃºc

### 4. Deployment
- Sá»­ dá»¥ng Light version
- ONNX export: tÄƒng tá»‘c inference
- TensorRT: optimize cho NVIDIA GPU
- Quantization: giáº£m model size xuá»‘ng 1/4

## ğŸ› Troubleshooting

### 1. Out of Memory
- Giáº£m `batch_size`
- Giáº£m `image_width`
- DÃ¹ng Light version
- Enable gradient checkpointing

### 2. Training khÃ´ng há»™i tá»¥
- Giáº£m learning rate
- TÄƒng warmup epochs
- Check data: cÃ³ bá»‹ corrupt khÃ´ng?
- Check vocab: cÃ³ Ä‘á»§ kÃ½ tá»± khÃ´ng?

### 3. Accuracy tháº¥p
- TÄƒng `svtr_depth`
- TÄƒng `hidden_size`
- Augmentation máº¡nh hÆ¡n
- Train lÃ¢u hÆ¡n

### 4. Inference cháº­m
- DÃ¹ng Light version
- Batch inference
- ONNX/TensorRT optimization
- Reduce image size

## ğŸ“š References

- **MobileNetV3**: [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)
- **SVTR**: [Scene Text Recognition with a Single Visual Model](https://arxiv.org/abs/2205.00159)
- **CTC**: [Connectionist Temporal Classification](https://www.cs.toronto.edu/~graves/icml_2006.pdf)

## ğŸ“œ License

Sá»­ dá»¥ng tá»± do cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

## ğŸ¤ Contributing

Náº¿u cÃ³ cáº£i tiáº¿n hoáº·c phÃ¡t hiá»‡n bug, vui lÃ²ng táº¡o issue hoáº·c pull request.

---

**Created with â¤ï¸ for Vietnamese Name Recognition**
